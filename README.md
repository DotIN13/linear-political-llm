# Probing Political Ideology in Large Language Models

**Replication Materials for EMNLP 2025 Findings Paper**
Tianyi Zhang – The University of Chicago
[Paper Link](https://aclanthology.org/2025.findings-emnlp.1267.pdf)


## Overview

This repository provides replication materials for:

> **“Probing Political Ideology in Large Language Models:
> How Latent Political Representations Generalize Across Tasks”**
> *Findings of the Association for Computational Linguistics: EMNLP 2025.*

The study investigates how large language models encode political ideology and whether latent ideological directions identified through linear probes generalize across reasoning tasks such as bias detection, simulated voting, and neutrality rewriting.

All replication code and analysis are contained in **`llama3.ipynb`**. Running this single notebook reproduces all results, figures, and tables from the paper.

## Repository Structure

```
LINEAR-POLITICAL-LLM/
├── data/                  # Input and processed datasets (downloaded or generated automatically)
├── figs/                   # Figures and visualizations saved by the notebook
├── results/               # Output files generated by the notebook
│
├── create_statements.py   # Optional helper script for statement synthesis
├── create_statements.sh   # Optional shell wrapper
├── create_vote_dataset.py # Optional helper for voting prompts
├── label_rewrites.py      # Optional helper for labeling rewritten text
│
├── llama3.ipynb           # Main replication notebook
├── requirements.txt
└── README.md
```

## Installation

```bash
git clone https://github.com/DotIN13/linear-political-llm.git
cd linear-political-llm

conda create -n linear-politics python=3.12 -y
conda activate linear-politics

pip install -r requirements.txt
```

### Note on Dependencies

This project uses a **customized version of the Hugging Face `transformers` library** available at
[https://github.com/DotIN13/transformers](https://github.com/DotIN13/transformers).

This version introduces a modification to the attention module by adding a
`self.head_out` module to each attention head.
The additional hook is used to extract and manipulate individual head activations for **probing** and **inference-time steering** experiments.

The custom version is automatically installed via `requirements.txt`, and no manual patching is required.

## Running the Notebook

1. Open **`llama3.ipynb`** in Jupyter or VS Code.
2. Run all cells sequentially.
   The notebook will:

   * Load or generate required datasets,
   * Conduct linear probing and inference-time interventions,
   * Evaluate tasks (bias detection, voting, rewriting),
   * Save outputs under `results/` and figures under `figs/`.

## Data

* **Synthetic Policy Statements:** Generated using templates aligned with the paper.
* **Voting Simulation Prompts:** Constructed to test intervention effects under ideological personas.
* **Rewriting Inputs:** Derived from the same policy statement set.

The dataset used in the study is publicly available at:
[Hugging Face – DotIN13/political-statements](https://huggingface.co/datasets/DotIN13/political-statements)

## Citation

If you use or build upon this work, please cite:

```bibtex
@inproceedings{zhang-2025-probing,
    title = "Probing Political Ideology in Large Language Models: How Latent Political Representations Generalize Across Tasks",
    author = "Zhang, Tianyi",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-emnlp.1267/",
    pages = "23349--23360",
    ISBN = "979-8-89176-335-7",
}
```

## Contact

Tianyi Zhang
Email: [tzhang3@uchicago.edu](mailto:tzhang3@uchicago.edu)
